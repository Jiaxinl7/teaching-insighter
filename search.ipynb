{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b19f83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh import index\n",
    "import os, os.path\n",
    "from whoosh import index\n",
    "from whoosh import qparser\n",
    "from whoosh.qparser import QueryParser\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "230fcce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(content_id=ID(stored=True), lesson=ID(stored=True), topic=TEXT(stored=True), content=TEXT(stored = True))\n",
    "\n",
    "# create empty index directory\n",
    "\n",
    "if not os.path.exists(\"index_dir\"):\n",
    "    os.mkdir(\"index_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b97522fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the schema to initialize a Whoosh index in the above directory\n",
    "\n",
    "ix = index.create_in(\"index_dir\", schema)\n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f0cc8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read topic content file, then process it\n",
    "df = pd.read_csv(\"cs510.csv\")\n",
    "i = 0\n",
    "new_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    all_sentence = str(row[\"TEXT\"]).split('.')\n",
    "    for sentence in all_sentence:\n",
    "        content_id = i\n",
    "        i += 1\n",
    "        lesson = row[\"Lesson\"]\n",
    "        topic = row[\"Topic\"]\n",
    "        content = sentence\n",
    "        if content != \"\":\n",
    "            while content[0] == ' ':\n",
    "                content = content[1:]\n",
    "            temp = [content_id, lesson, topic, content]\n",
    "            new_list.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a049a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in new_list:\n",
    "    content_id = row[0]\n",
    "    lesson = row[1]\n",
    "    topic = row[2]\n",
    "    content = row[3]\n",
    "    writer.add_document(content_id=str(content_id), lesson=str(lesson), topic=str(topic), content=str(content))\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c2eb7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "def index_search(dirname, search_fields, search_query):\n",
    "    ix = index.open_dir(dirname)\n",
    "    schema = ix.schema\n",
    "    \n",
    "    og = qparser.OrGroup.factory(0.9)\n",
    "    mp = qparser.MultifieldParser(search_fields, schema, group = og)\n",
    "\n",
    "    \n",
    "    q = mp.parse(search_query)\n",
    "    \n",
    "    \n",
    "    with ix.searcher() as s:\n",
    "        results = s.search(q, terms=True, limit = 5)\n",
    "#         print(\"Search Results: \")\n",
    "        if results.is_empty():\n",
    "            result_list.append(\"\")\n",
    "        else:\n",
    "            result_list.append(results[0]['content'])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b2abd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index_search(\"index_dir\", ['topic', 'content'], \"Since we avoid the too frequent words (e.g., 'the') for the initial idea, why not avoid them also in mixture LM examples by ignoring the less valuable words and smooth the frequency of other words likewise?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15b762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "145316ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are several major types of language models, including statistical language models, rule-based language models, and neural language models', 'An N-gram language model is a statistical language model that uses a sequence of N words (an N-gram) to predict the next word in a text or speech', 'A statistical language model is a mathematical model that estimates the probability of a sequence of words', 'The model is trained using a set of training data, and the resulting model can be used to predict the probability of a sequence of hidden states', 'Cross entropy is a measure of the difference between two probability distributions', 'Mutual information is a measure of the amount of information that two variables share', 'Cross entropy is a measure of the difference between two probability distributions', 'Cross entropy is a measure of the difference between two probability distributions', 'One is that the probability of a word occurring is only dependent on the words that come before it', 'There are a few assumptions that are made in an N-gram language model', 'To interpolate a value, we first need two known values', 'The cross entropy can be thought of as a way of quantifying the amount of information that is lost when using distribution Q to approximate distribution P', 'The idea is to discount the importance of older observations, while still incorporating them into the estimate', 'For example, if an event was seen 10 times in training data, then the estimated probability of that event is: P(event) = 10 / (10 + 1) = 0', 'It is calculated as the amount of information that one variable contains about the other, divided by the total amount of information in both variables', '', 'Relative entropy is a measure of how different two probability distributions are', 'Entropy is typically measured in bits, and can be calculated using the following equation: Entropy = -log2(P), where P is the probability of each event occurring', 'There are several major types of language models, including statistical language models, rule-based language models, and neural language models', 'An N-gram language model is a statistical language model that uses a sequence of N words (an N-gram) to predict the next word in a text or speech', 'KL-divergence is a measure of how different two probability distributions are', 'An N-gram language model is a statistical language model that uses a sequence of N words (an N-gram) to predict the next word in a text or speech', 'Cross entropy is a measure of the difference between two probability distributions', 'An N-gram language model is a statistical language model that uses a sequence of N words (an N-gram) to predict the next word in a text or speech', 'If two distributions are identical, the relative entropy is zero', 'KL-divergence is a measure of how different two probability distributions are', 'A statistical language model is a mathematical model that estimates the probability of a sequence of words', 'Plugging in our values, we get: y = ((3-1)/(5-1))*(5-1) + 1, y = 2', 'A statistical language model is a mathematical model that estimates the probability of a sequence of words', 'There are several major types of language models, including statistical language models, rule-based language models, and neural language models', 'Relative entropy is a measure of how different two probability distributions are', 'The ML estimate of an N-gram language model is the probability of a sequence of words occurring, given the frequencies of individual words and the length of the sequence', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'Absolute discount smoothing is a method for estimating the probability of an event occurring based on past observations', 'Smoothing is a technique used to reduce the impact of data sparsity in language models', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'Dirichlet prior smoothing is a technique used to smooth out data that has been collected from a text corpus', 'The ML estimate of an N-gram language model is the probability of a sequence of words occurring, given the frequencies of individual words and the length of the sequence', 'Relative entropy is a measure of how different two probability distributions are', 'The Viterbi algorithm can be used for any type of HMM, but it is particularly well-suited for applications where the number of hidden states is large or unknown, or where the transition probabilities between states are very small', 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'These models are often used in conjunction with other methods, such as latent semantic analysis, in order to improve the accuracy of the predictions', 'The Good Turing estimator is based on the idea that the probability of an event is proportional to the number of times it is seen in training data', 'Additive smoothing is a technique used to smooth out data by adding a small constant value to each data point', 'The Good Turing estimator is based on the idea that the probability of an event is proportional to the number of times it is seen in training data', 'The ML estimate of an N-gram language model is the probability of a sequence of words occurring, given the frequencies of individual words and the length of the sequence', 'The ML estimate of an N-gram language model is the probability of a sequence of words occurring, given the frequencies of individual words and the length of the sequence', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'The unigram language model is the simplest form of statistical language modeling, and is often used as a baseline for more complex models', 'For example, if an event was seen 10 times in training data, then the estimated probability of that event is: P(event) = 10 / (10 + 1) = 0', 'The unigram language model is the simplest form of statistical language modeling, and is often used as a baseline for more complex models', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'Dirichlet prior smoothing is a technique used to smooth out data that has been collected from a text corpus', 'Two stage smoothing is a technique used to improve the accuracy of forecasts by combining information from multiple sources', 'Dirichlet prior smoothing is a technique used to smooth out data that has been collected from a text corpus', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'Two stage smoothing is a technique used to improve the accuracy of forecasts by combining information from multiple sources', 'Relative entropy is a measure of how different two probability distributions are', 'There are several major types of language models, including statistical language models, rule-based language models, and neural language models', 'To interpolate a value at point C, which has an x-value of 3, we would use the following equation: y = ((x-A)/(B-A))*(B-A) + A', 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'The model is trained using a set of training data, and the resulting model can be used to predict the probability of a sequence of hidden states', 'There are many different types of smoothing techniques, including Laplace smoothing, Lidstone smoothing, and Jelinek-Mercer smoothing', 'Two stage smoothing is a technique used to improve the accuracy of forecasts by combining information from multiple sources', 'It is often used to estimate data points that are irregularly spaced or missing', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'If they are different, the relative entropy is positive', 'Plugging in our values, we get: y = ((3-1)/(5-1))*(5-1) + 1, y = 2', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', 'LDA is a probabilistic model, which means that it uses probability to determine the hidden topics in the documents', 'One is that the probability of a word occurring is only dependent on the words that come before it', 'Relative entropy is a measure of how different two probability distributions are', 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'The context vector can be used to represent any kind of contextual information about the document, such as the time, place, or author', 'The idea is to discount the importance of older observations, while still incorporating them into the estimate', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', '', 'The transition probabilities are updated using the forward-backward algorithm, while the emission probabilities are updated using the maximum likelihood estimate', 'To interpolate a value at point C, which has an x-value of 3, we would use the following equation: y = ((x-A)/(B-A))*(B-A) + A', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', 'This allows the model to better generalize to new data', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', 'The model can be trained using an expectation-maximization algorithm', 'Cross entropy is a measure of the difference between two probability distributions', 'Relative entropy is a measure of how different two probability distributions are', 'Cross entropy is a measure of the difference between two probability distributions', \"The model is based on the principle of maximum likelihood, which states that the probability of a document being relevant to a query is proportional to the likelihood of the document's contents given the query\", 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'To interpolate a value, we first need two known values', 'Absolute discount smoothing is a method for estimating the probability of an event occurring based on past observations', '', 'There are several major types of language models, including statistical language models, rule-based language models, and neural language models', 'This constant is typically a very small number, such as 0', 'The higher the entropy, the more information that is contained in the data set', 'The general form of the likelihood function of a mixture model is given by: L(x|θ) = ∑i=1Np(i)f(x|θi), where x is the data, θ is the parameter vector, N is the number of mixture components, and p(i) is the weight of the i-th component', 'A mixture language model is a type of statistical language model that uses a mixture of different sub-models to better capture the structure of a given language', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'Contextual topic modeling extends LDA by adding a context vector to the model', 'Absolute discount smoothing is a method for estimating the probability of an event occurring based on past observations', 'The context vector can be used to represent any kind of contextual information about the document, such as the time, place, or author', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'Relative entropy is a measure of how different two probability distributions are', 'LDA can be used for information retrieval by first creating a set of documents, and then using LDA to discover the hidden topics in those documents', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', 'Dirichlet prior smoothing is a technique used to smooth out data that has been collected from a text corpus', 'The technique is based on the idea of latent Dirichlet allocation (LDA), which is a generative model for text data', 'Plugging in our values, we get: y = ((3-1)/(5-1))*(5-1) + 1, y = 2', 'Dirichlet prior smoothing is a technique used to smooth out data that has been collected from a text corpus', 'The resulting model can then be used to generate embeddings for words in new text documents', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'PLSA is a statistical model that can be used to represent the co-occurrence of terms in a document', 'This is done by adding a small amount of noise to the data points, which is typically done by adding a Dirichlet distribution with a small parameter value to the data', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'To interpolate a value at point C, which has an x-value of 3, we would use the following equation: y = ((x-A)/(B-A))*(B-A) + A', 'One is that the probability of a word occurring is only dependent on the words that come before it', 'The algorithm is named after its inventors, Leonard Baum and Ted Welch', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'The unigram language model is the simplest form of statistical language modeling, and is often used as a baseline for more complex models', 'The model is composed of a set of hidden states, a set of observable states, and a set of transition probabilities between the hidden states', 'The algorithm is named after its inventors, Leonard Baum and Ted Welch', 'The Viterbi algorithm can be used for any type of HMM, but it is particularly well-suited for applications where the number of hidden states is large or unknown, or where the transition probabilities between states are very small', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'Contextual topic modeling extends LDA by adding a context vector to the model', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'The forward algorithm is used to calculate the probability of a sequence of observations, given a hidden Markov model', 'The model is trained using a set of training data, and the resulting model can be used to predict the probability of a sequence of hidden states', 'The individual sub-models can be based on different types of data, such as different parts of speech or different frequency ranges', 'The model is composed of a set of hidden states, a set of observable states, and a set of transition probabilities between the hidden states', 'The Viterbi algorithm can be used for any type of HMM, but it is particularly well-suited for applications where the number of hidden states is large or unknown, or where the transition probabilities between states are very small', 'Probabilistic retrieval models are a type of information retrieval model that uses probabilities to predict how likely it is that a given document will be relevant to a given query', 'The model is based on the assumption that there is a latent semantic structure in the collection of documents being searched, and that this structure can be represented as a vector space', 'The Baum-Welch algorithm converges to a local optimum of the likelihood function, so it is important to start the algorithm with good initial estimates of the parameters', 'Word2vec is a neural network model for learning word embeddings, which are vector representations of words that can be used for various natural language processing tasks', 'ELMo is a deep learning algorithm that stands for Embeddings from Language Models', 'The model is based on the Transformer architecture and can be used for a variety of tasks, such as text classification, question answering, and machine translation', 'Dirichlet prior smoothing is a technique used to smooth out data that has been collected from a text corpus', 'is commonly used in information retrieval tasks', 'ELMo is a deep learning algorithm that stands for Embeddings from Language Models']\n"
     ]
    }
   ],
   "source": [
    "questions = pd.read_csv(\"510questions.csv\")\n",
    "all_result = []\n",
    "for _, row in questions.iterrows():\n",
    "    index_search(\"index_dir\", ['topic', 'content'], row[\"questions\"])\n",
    "    \n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9acd742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['answer'] = result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c0bf2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.to_csv('questions_with_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471713b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
